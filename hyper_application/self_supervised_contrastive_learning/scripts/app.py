import os.path
import streamlit as st
import onnxruntime as ort
import numpy as np
import torch
from torchvision.transforms import ToTensor
from pre_processing import load_data
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Load ONNX model
def load_onnx_model(model_path="scripts/models/model_1.onnx"):
    session = ort.InferenceSession(os.path.join(os.getcwd(),model_path))
    return session

# Run inference with ONNX model
def run_inference(session, input_image):
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    # Prepare the input image (convert to NumPy array)
    input_image = input_image.numpy()
    input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension
    input_image = input_image.astype(np.float32)

    # Perform inference
    outputs = session.run([output_name], {input_name: input_image})
    return outputs[0]  # Return the output embeddings

# Visualize embeddings using t-SNE
def visualize_embeddings(session, data_loader):
    embeddings = []
    labels = []

    for images, targets in data_loader:
        for image, label in zip(images, targets):
            embedding = run_inference(session, image)
            embeddings.append(embedding)
            labels.append(label.item())

    embeddings = np.vstack(embeddings)
    labels = np.array(labels)

    # Reduce dimensions using t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    reduced_embeddings = tsne.fit_transform(embeddings)

    return reduced_embeddings, labels

def streamlit_ui():
    # Streamlit UI
    st.title("ONNX Model Visualization for SimpleCLR")

    st.info("""
        ### About This Project
        This application enables you to visualize and understand machine learning embeddings produced by an ONNX-based **SimCLR** model. It provides insights into the model's learning process through interactive visualizations.

        **Key Features**:
        - **Dataset Preview**: Explore and visualize raw and augmented images from the dataset.
        - **Embedding Visualization**: Analyze the learned embeddings using **t-SNE** for dimensionality reduction.
        - **Original vs Augmented**: Compare embeddings generated from the original and augmented images to understand the model's contrastive learning process.

        **How It Works**:
        1. Select the dataset (Train/Test) you want to explore.
        2. Generate and visualize embeddings using the ONNX model.
        3. Compare original and augmented image embeddings to evaluate SimCLR's performance.

        **Who Is It For?**
        Data scientists, machine learning practitioners, and researchers aiming to understand or debug contrastive learning models like SimCLR.
        """)
    
    st.sidebar.title("Navigation")
    options = ["Dataset Preview", "Embedding Visualization", "Original vs Augmented"]
    selection = st.sidebar.selectbox("Select a Visualization", options)

    # Load ONNX model and dataset
    session = load_onnx_model()
    train_data, test_data = load_data()

    if selection == "Dataset Preview":
        st.header("Dataset Preview")
        data_type = st.selectbox("Select Dataset", ["Train Data", "Test Data"])
        data_loader = train_data if data_type == "Train Data" else test_data

        # Show a batch of images
        images, labels = next(iter(data_loader))
        fig, axs = plt.subplots(4, 4, figsize=(10, 10))
        classes = data_loader.dataset.classes
        for i, ax in enumerate(axs.flatten()):
            ax.imshow(images[i].permute(1, 2, 0))
            ax.set_title(f"Label: {classes[labels[i]]}")
            ax.axis("off")
        st.pyplot(fig)

    elif selection == "Embedding Visualization":
        st.header("Embedding Visualization")

        # Dropdown to select dataset
        data_type = st.selectbox("Select Dataset", ["Train Data", "Test Data"])
        data_loader = train_data if data_type == "Train Data" else test_data

        # Generate embeddings with a spinner for user feedback
        with st.spinner("Generating embeddings...Please wait it usually takes 2-3 minutes"):
            reduced_embeddings, labels = visualize_embeddings(session, data_loader)

        # Plot embeddings
        plt.figure(figsize=(8, 8))
        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap="tab10", alpha=0.7)
        plt.colorbar(scatter, label="Class Labels")
        plt.title(f"{data_type} Embeddings (t-SNE Projection)")
        st.pyplot(plt)

        # Explanation for the visualization
        st.subheader("What Does This Visualization Represent?")
        st.markdown(
            """
            This plot visualizes the dataset's embeddings, which are compact numerical representations of the data points. 
            These embeddings are generated by processing the dataset through a machine learning model.

            The embeddings are reduced to two dimensions using a technique called **t-SNE (t-distributed Stochastic Neighbor Embedding)**. 
            This dimensionality reduction allows us to plot high-dimensional data on a 2D plane while preserving relationships between data points.

            ### Key Details:
            - **Colored Points:** Each dot represents a data point in the dataset, and the color indicates its class or label.
            - **Clusters:** Points that are close together in the plot are similar according to the model, while points far apart are less similar.
            - **Purpose:** This visualization helps us understand how well the model separates or clusters different classes of data.

            ### How to Interpret:
            - If you see distinct clusters with minimal overlap, it means the model is effectively learning to differentiate between the classes.
            - Overlapping or scattered points might indicate that the model struggles to distinguish between certain classes.

            This visualization provides insights into the inner workings of the model, making it easier to debug or improve its performance.
            """
        )


    elif selection == "Original vs Augmented":
        st.header("Original vs Augmented Representations")
        st.write("Compare original and augmented image representations.")

        # Select an image from test data
        images, labels = next(iter(test_data))
        index = st.slider("Select Image Index", 0, len(images) - 1, 0)

        # Original and augmented images
        original_image = images[index].permute(1, 2, 0)
        augmented_image = torch.flip(images[index], [1]).permute(1, 2, 0)

        # Model embeddings
        original_embedding = run_inference(session, images[index])
        augmented_embedding = run_inference(session, torch.flip(images[index], [1]))

        col1, col2 = st.columns(2)

        with col1:
            # Plot the original image using matplotlib
            fig, ax = plt.subplots()
            ax.imshow(original_image)
            ax.axis('off')  # Turn off axis labels
            ax.set_title(f"Original Image (Label: {labels[index]})")
            st.pyplot(fig)  # Render the matplotlib figure in Streamlit
            st.write(
                "This is the **original image** as it was captured. The model learns patterns like shapes, colors, "
                "and textures from this version.")


        with col2:
            # Plot the augmented image using matplotlib
            fig, ax = plt.subplots()
            ax.imshow(augmented_image)
            ax.axis('off')  # Turn off axis labels
            ax.set_title("Augmented Image (Flipped)")
            st.pyplot(fig)  # Render the matplotlib figure in Streamlit
            st.write(
                "This is the **flipped version** of the original image. "
                "The model sees this as a variation and learns to recognize it as the same object despite the flip.")

        st.subheader("What Does This Mean for SimCLR?")
        st.markdown(
            """
            SimCLR is a model that learns to recognize objects by comparing different versions of the same image. 
            Here’s how it processes and learns from the original and augmented images:

            ### Key Details:
            - **Original Image:**
              - SimCLR learns the **key features** of the object in the original image, such as:
                - Overall shape
                - Colors
                - Textures
              - **Example:** If the image is of a cat, the model might focus on the fur texture, eyes, and ears to understand what makes it a "cat."

            - **Augmented Image (Flipped):**
              - The flipped version is still the same object (e.g., a cat), but it appears slightly different.
              - SimCLR **learns to recognize that it’s the same object** even when its orientation or appearance changes.

            ### Why Is This Important?
            - This approach helps the model become smarter and more flexible, enabling it to recognize objects in various real-world scenarios.
            - For instance, a cat lying down, flipped horizontally, or seen from another angle should still be recognized as a cat.

            By learning from these variations, SimCLR becomes robust and adaptable, allowing it to generalize well across unseen data.
            """
        )


if __name__ == "__main__":
    streamlit_ui()
